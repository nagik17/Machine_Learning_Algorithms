{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sNKZq4XrXQh"
   },
   "source": [
    "# <font color='red'><b>Bootstrap assignment</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAHap1Z3FZC-"
   },
   "source": [
    "<b>There will be some functions that start with the word \"grader\" ex: grader_sampples(), grader_30().. etc, you should not change those function definition.\n",
    "\n",
    "Every Grader function has to return True.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuxBq_bvrwh2"
   },
   "source": [
    "<font color='blue'> <b>Importing packages</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6ag91ijrQOs"
   },
   "outputs": [],
   "source": [
    "import numpy as np # importing numpy for numerical computation\n",
    "from sklearn.datasets import load_boston # here we are using sklearn's boston dataset\n",
    "from sklearn.metrics import mean_squared_error # importing mean_squared_error metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcHOsONTt1K_"
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "x=boston.data #independent variables\n",
    "y=boston.target #target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pc1htEFYuLRj",
    "outputId": "f5b60712-98b3-4cdc-b629-3546c1e3859c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "kQle3T_wuOa3",
    "outputId": "521c7bdd-5316-48d5-c534-b61d170d2c28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n",
       "        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02,\n",
       "        1.5300e+01, 3.9690e+02, 4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9690e+02, 9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9283e+02, 4.0300e+00],\n",
       "       [3.2370e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,\n",
       "        6.9980e+00, 4.5800e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02,\n",
       "        1.8700e+01, 3.9463e+02, 2.9400e+00],\n",
       "       [6.9050e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,\n",
       "        7.1470e+00, 5.4200e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02,\n",
       "        1.8700e+01, 3.9690e+02, 5.3300e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEa_HqRZloH4"
   },
   "source": [
    "## <font color='red'><b>Task 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQ5q8IxHNRk3"
   },
   "source": [
    "<font color='red'> <b>Step - 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJCFCaOzl7Mr"
   },
   "source": [
    "*  <font color='blue'><b>Creating samples</b></font><br>\n",
    "    <b> Randomly create 30 samples from the whole boston data points</b>\n",
    "    *  Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points\n",
    "    \n",
    "     For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly , consider we have selected [4, 5, 7, 8, 9, 3] now we will replicate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]\n",
    "* <font color='blue'><b> Create 30 samples </b></font>\n",
    "    *  Note that as a part of the Bagging when you are taking the random samples <b>make sure each of the sample will have different set of columns</b><br>\n",
    "Ex: Assume we have 10 columns[1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10] for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample  [7, 9, 1, 4, 5, 6, 2] and so on...\n",
    "Make sure each sample will have atleast 3 feautres/columns/attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUqFEBSvNjCa"
   },
   "source": [
    "<font color='red'><b>Step - 2 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uqi9AhCYNq3Z"
   },
   "source": [
    "<font color='blue'><b>Building High Variance Models on each of the sample and finding train MSE value</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lLBnZHXOFln"
   },
   "source": [
    "*  Build a regression trees on each of 30 samples.\n",
    "*  Computed the predicted values of each data point(506 data points) in your corpus.\n",
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$\n",
    "*  Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kls23JLnSN23"
   },
   "source": [
    "<font color='red'> <b>Step - 3 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rz2GchkGSWnh"
   },
   "source": [
    "*  <font color='blue'><b>Calculating the OOB score </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGHkVV2kSibm"
   },
   "source": [
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.\n",
    "*  Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK860ocxTyoz"
   },
   "source": [
    "# <font color='red'><b>Task 2</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dme-N6TUCrY"
   },
   "source": [
    "*  <font color='blue'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
    "  *   Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
    "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
    "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
    "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
    "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6UcH1x9Uwrj"
   },
   "source": [
    "# <font color='red'><b>Task 3</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOC_AgsLU7OH"
   },
   "source": [
    "*  <font color='blue'><b>Given a single query point predict the price of house.</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYs5jSFdVILe"
   },
   "source": [
    "Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] \n",
    "Predict the house price for this point as mentioned in the step 2 of Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6gxZEsFWm-8"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2fHTdS_zpgG"
   },
   "source": [
    "# <font color='blue'> <b>Task - 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0yGBuryOwHz"
   },
   "source": [
    "<font color='blue'><b>Step - 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJXX8vf3z073"
   },
   "source": [
    "*  <font color='blue'> <b>Creating samples</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSVaWG1F4uCZ"
   },
   "source": [
    "<font color='Orange'><b>Algorithm</b></font>\n",
    "\n",
    "![alt text](https://i.imgur.com/BTVYXQ1.jpg/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_oWoN97BhDY"
   },
   "source": [
    "*  <font color='blue'><b> Write code for generating samples</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ph_6D2SDzz7F"
   },
   "outputs": [],
   "source": [
    "def generating_samples(input_data, target_data):\n",
    "\n",
    "    '''In this function, we will write code for generating 30 samples '''\n",
    "    # you can use random.choice to generate random indices without replacement\n",
    "    # Please have a look at this link https://docs.scipy.org/doc/numpy-1.16.0/reference/generated/numpy.random.choice.html for more details\n",
    "    # Please follow above pseudo code for generating samples \n",
    "\n",
    "    # return sampled_input_data , sampled_target_data,selected_rows,selected_columns\n",
    "    #note please return as lists\n",
    "    selecting_rows = np.random.choice(len(input_data), size=303, replace=False) \n",
    "    replicating_rows = np.random.choice(len(selecting_rows),size= 203,replace=True)\n",
    "    col = np.arange(3,len(input_data[1]),1)\n",
    "    no_of_col = np.random.choice(col)\n",
    "    range_col=  [*range(0,13,1)]\n",
    "    selecting_columns = np.random.choice(range_col,size=no_of_col, replace=False)\n",
    "    sample_data = input_data[selecting_rows[:, None],selecting_columns]\n",
    "    target_sample_data = target_data[selecting_rows]\n",
    "    ############ \n",
    "    # replicating sample data\n",
    "    replicating_sample_data = sample_data[replicating_rows]\n",
    "    target_replicated_sample_data = target_data[replicating_rows]\n",
    "    ###########\n",
    "    # concatnating data\n",
    "    final_sample_data = np.vstack((sample_data,replicating_sample_data))\n",
    "    final_target_data = np.vstack((target_sample_data.reshape(-1,1),target_replicated_sample_data.reshape(-1,1)))\n",
    "    \n",
    "    return list(final_sample_data),list(final_target_data),list(selecting_rows),list(selecting_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MivEQFlm7iOg"
   },
   "source": [
    "<font color='cyan'> <b> Grader function - 1 </b> </fongt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVvuhNzm7uld"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_samples(a,b,c,d):\n",
    "    length = (len(a)==506  and len(b)==506)\n",
    "    sampled = (len(a)-len(set([str(i) for i in a]))==203)\n",
    "    rows_length = (len(c)==303)\n",
    "    column_length= (len(d)>=3)\n",
    "    assert(length and sampled and rows_length and column_length)\n",
    "    return True\n",
    "a,b,c,d = generating_samples(x, y)\n",
    "grader_samples(a,b,c,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4LSsmn4Jn2_"
   },
   "source": [
    "*  <font color='blue'> <b>Create 30 samples </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ec7MN6sL2BZ"
   },
   "source": [
    "![alt text](https://i.imgur.com/p8eZaWL.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXlKWjCcBvTk"
   },
   "outputs": [],
   "source": [
    "# Use generating_samples function to create 30 samples \n",
    "# store these created samples in a list\n",
    "list_input_data =[]\n",
    "list_output_data =[]\n",
    "list_selected_row= []\n",
    "list_selected_columns=[]\n",
    "for i in range(0,30):\n",
    "    a,b,c,d = generating_samples(x,y)\n",
    "    list_input_data.append(a)\n",
    "    list_output_data.append(b)\n",
    "    list_selected_row.append(c)\n",
    "    list_selected_columns.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXUz9VFiMQkh"
   },
   "source": [
    "<font color='cyan'> <b>Grader function - 2 </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCvIq8NuMWOC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_30(a):\n",
    "    assert(len(a)==30 and len(a[0])==506)\n",
    "    return True\n",
    "grader_30(list_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Pv-mkZkO6dh"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whaHCPB0O8qF"
   },
   "source": [
    "<font color='red'><b>Step - 2 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XBy4zXSWPtU8"
   },
   "source": [
    "<font color='orange'><b>Flowchart for building tree</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xvH06HPQBdP"
   },
   "source": [
    "![alt text](https://i.imgur.com/pcXfSmp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRwPO_uHQjul"
   },
   "source": [
    "*  <font color='blue'><b> Write code for building regression trees</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWQp6tRwMthq"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "input_data =[]\n",
    "output_data=[]\n",
    "list_of_models=[]\n",
    "for i in range(30):\n",
    "    input_data.append(x[np.asarray(list_selected_row[i])[:,None], np.asarray(list_selected_columns[i])[None,:]])\n",
    "    output_data.append(y[np.asarray(list_selected_row[i])[:,None]])\n",
    "    model = DecisionTreeRegressor(max_depth=None)\n",
    "    model.fit(input_data[i],output_data[i])\n",
    "    list_of_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=None, splitter='best')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21j8BKfAQ1U8"
   },
   "source": [
    "<font color='orange'><b>Flowchart for calculating MSE </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Q0mTBD2RBx_"
   },
   "source": [
    "![alt text](https://i.imgur.com/sPEE618.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6e-UamlHRjPy"
   },
   "source": [
    "After getting predicted_y for each data point, we can use sklearns mean_squared_error to calculate the MSE between predicted_y and actual_y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnIMT7_oR312"
   },
   "source": [
    "*  <font color='blue'><b> Write code for calculating MSE</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWhcvMRWRA9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE:\n",
      "0.03526679841897236\n"
     ]
    }
   ],
   "source": [
    "y_pred=[]\n",
    "for i in range(506):\n",
    "    array_for_y =[]\n",
    "    for j in range(30):\n",
    "        array_for_y.append(list_of_models[j].predict(x[i][list_selected_columns[j]].reshape(1,-1)))\n",
    "    y_pred.append(np.median(array_for_y))\n",
    "\n",
    "\n",
    "print(\"The MSE:\")\n",
    "diff=[]\n",
    "for i in range(506):\n",
    "    diff.append((y[i]-y_pred[i])**2)\n",
    "print(np.sum(diff)/506)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuclPDMnSz8F"
   },
   "source": [
    "<font color='blue'><b>Step - 3 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESb9FSIDTM5V"
   },
   "source": [
    "<font color='orange'><b>Flowchart for calculating OOB score</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HB-d6NMETbd9"
   },
   "source": [
    "![alt text](https://i.imgur.com/95S5Mtm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WW3GOcFzTqbt"
   },
   "source": [
    "Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBqcS03pUYSZ"
   },
   "source": [
    "*  <font color='blue'><b> Write code for calculating OOB score </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fog_6DNdS-h_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OOB Score:\n",
      "16.288305335968378\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "y_pred1=[]\n",
    "for i in range(506):\n",
    "    array_for_y_new =[]\n",
    "    for j in range(30):\n",
    "        if i not in list_selected_row[j]:\n",
    "            array_for_y_new.append(list_of_models[j].predict(x[i][list_selected_columns[j]].reshape(1,-1)))  \n",
    "    y_pred1.append(np.median(array_for_y_new))\n",
    "\n",
    "\n",
    "print(\"The OOB Score:\")\n",
    "diff1=[]\n",
    "for i in range(506):\n",
    "    diff1.append((y[i]-y_pred1[i])**2)\n",
    "print(np.sum(diff1)/506)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbuiwX3OUjUI"
   },
   "source": [
    "# <font color='blue'><b>Task 2</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ceW5-D88Uswi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 35/35 [01:42<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.22413921826965313, 0.026246603260869566, 0.07922430830039523, 0.011329051383399194, 0.15234574000878343, 0.028379446640316247, 0.4535140810276678, 0.11630327371693992, 0.1967008399209486, 0.29019762845849806, 0.09386487154150201, 0.15107240338164263, 0.36606952129995607, 0.11937005928853762, 0.035454545454545516, 0.1347776679841897, 0.1894577569169961, 0.07993083003952571, 0.21189723320158108, 0.03809288537549412, 0.0973381916996047, 0.026541501976284597, 0.033414031620553376, 0.016679841897233202, 0.1352371541501976, 0.06391921936758896, 0.062223320158102655, 0.3039538043478262, 0.0008053359683794527, 0.08194883618796664, 0.5897903285573122, 0.05143280632411064, 0.06268774703557313, 0.04146739130434788, 0.12012957393925955]\n",
      "####################################################################################################\n",
      "OOB: [13.883775563287035, 13.884888833992095, 14.988275828941587, 13.944403958058846, 17.343254861252984, 20.776191509861313, 16.308720355731225, 12.564846297893045, 15.704843723125364, 15.610796689723319, 14.441715616947866, 15.302368011790612, 15.42158483460053, 14.8846295838823, 14.146809947299078, 14.757085289031622, 13.744259853974526, 14.798485382924339, 9.895015130928854, 17.056981533694245, 14.042672152120357, 16.20730144413544, 16.02696456472942, 12.645247035573124, 15.03674492890865, 11.075089673913043, 17.03412055335968, 12.342551312094203, 17.10304114514712, 17.62219863474723, 19.47426563763309, 14.682272727272727, 15.923053359683792, 12.215656067761971, 15.907911504802557]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "mse=[];oob=[];\n",
    "for i in tqdm.tqdm(range(35)):\n",
    "    list_input_data =[]\n",
    "    list_output_data =[]\n",
    "    list_selected_row= []\n",
    "    list_selected_columns=[]\n",
    "    for i in range(0,30):\n",
    "        a,b,c,d = generating_samples(x,y)\n",
    "        list_input_data.append(a)\n",
    "        list_output_data.append(b)\n",
    "        list_selected_row.append(c)\n",
    "        list_selected_columns.append(d)\n",
    "    input_data =[]\n",
    "    output_data=[]\n",
    "    list_of_models=[]\n",
    "    for i in range(30):\n",
    "        input_data.append(x[np.asarray(list_selected_row[i])[:,None], np.asarray(list_selected_columns[i])[None,:]])\n",
    "        output_data.append(y[np.asarray(list_selected_row[i])[:,None]])\n",
    "        model = DecisionTreeRegressor(max_depth=None)\n",
    "        model.fit(input_data[i],output_data[i])\n",
    "        list_of_models.append(model)\n",
    "    y_pred=[]\n",
    "    for i in range(506):\n",
    "        array_for_y =[]\n",
    "        for j in range(30):\n",
    "            array_for_y.append(list_of_models[j].predict(x[i][list_selected_columns[j]].reshape(1,-1)))\n",
    "        y_pred.append(np.median(array_for_y))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    y_pred1=[]\n",
    "    for i in range(506):\n",
    "        array_for_y_new =[]\n",
    "        for j in range(30):\n",
    "            if i not in list_selected_row[j]:\n",
    "                array_for_y_new.append(list_of_models[j].predict(x[i][list_selected_columns[j]].reshape(1,-1)))  \n",
    "        y_pred1.append(np.median(array_for_y_new))\n",
    "    mse.append(mean_squared_error(y,y_pred))\n",
    "    oob.append(mean_squared_error(y,y_pred1))\n",
    "print(\"MSE:\",mse)\n",
    "print('#'*100)\n",
    "print(\"OOB:\",oob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Confidence Interval CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confidence Interval of MSE:\n",
      "[ 0.0899347151986333 , 0.17783311623026857 ]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "sample_mean_mse = np.mean(mse)\n",
    "sample_std_mse = np.std(mse)\n",
    "n = len(mse)\n",
    "standard_error_mean_mse = sample_std_mse/(math.sqrt(n))\n",
    "print(\"The Confidence Interval of MSE:\")\n",
    "print('[',sample_mean_mse-(2*standard_error_mean_mse),',',sample_mean_mse+(2*standard_error_mean_mse),']')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confidence Interval of OOB:\n",
      "[ 14.32721837538061 , 15.775525827409282 ]\n"
     ]
    }
   ],
   "source": [
    "sample_mean_oob = np.mean(oob)\n",
    "sample_std_oob = np.std(oob)\n",
    "m = len(oob)\n",
    "standard_error_mean_oob = sample_std_oob/(math.sqrt(m))\n",
    "print(\"The Confidence Interval of OOB:\")\n",
    "print('[',sample_mean_oob-(2*standard_error_mean_oob),',',sample_mean_oob+(2*standard_error_mean_oob),']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKTnJdiBVS_e"
   },
   "source": [
    "# <font color='blue'><b>Task 3</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXxrvZqHV1Fr"
   },
   "source": [
    "<font color='orange'><b>Flowchart for Task 3</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyjwEJ62V6a6"
   },
   "source": [
    "<b>Hint: </b> We created 30 models by using 30 samples in TASK-1. Here, we need send query point \"xq\"  to 30 models and perform the regression on the output generated by 30 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0emSwLL7VurD"
   },
   "source": [
    "![alt text](https://i.imgur.com/Y5cNhQk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29hjwKlWWDfo"
   },
   "source": [
    "*  <font color='blue'><b> Write code for TASK 3 </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_pUlSD-VYD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value for the given xq:  18.9\n"
     ]
    }
   ],
   "source": [
    "xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "yq=[]\n",
    "array_for_y_q =[]\n",
    "for j in range(30):\n",
    "    if i not in list_selected_row[j]:\n",
    "         array_for_y_q.append(list_of_models[j].predict(np.array(xq)[list_selected_columns[j]].reshape(1,-1)))  \n",
    "yq.append(np.median(array_for_y_q))\n",
    "print('The predicted value for the given xq: ',float(np.asarray(yq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IOdUi-0xWOJ9"
   },
   "source": [
    "<font color='red'><b>Write observations for task 1, task 2, task 3 indetail</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIcax45hWKT-"
   },
   "source": [
    "#### TASK 1: \n",
    "The Boston Dataset is used here which has prices of houses from various locations. It contains 506 rows and 13 features(crime-rate,age,etc) <br /> \n",
    "Here I apply Bootstrapping method and then use DecisionTreeRegressor as our model. <br /> \n",
    "By using the function \"generating_samples\" we obtain a sample dataset from the actual given data. <br /> \n",
    "The function returns a sample data of size 506 with number of columns ranging from 3 to 13. <br /> \n",
    "We create 30 samples ensuring each sample has different set of columns. <br /> \n",
    "<br /> \n",
    "\n",
    "By using DecisionTreeRegressor() models fit on input data column sampled on each of the 30 samples, <br /> \n",
    "I found MSE = 0.03526679841897236 <br /> \n",
    "and Out of Bag(oob) Score = 16.288305335968378 <br /> \n",
    "<br /> \n",
    "\n",
    "#### TASK 2:\n",
    "Here, I generate 35 samples and obtain 35 MSE and OOB values in the form of a list. <br /> \n",
    "\n",
    "Next I calculate the Confidence Intervals of MSE & OOB Score taking above two lists as samples.<br /> \n",
    "By refering the Central_limit_theorem notebook, <br /> \n",
    "since we have no population std_dev statistic, we apply the case 2 of the conclusion given in above mentioned notebook. <br /> \n",
    "I have obtained: <br /> \n",
    "The Confidence Interval of MSE: <br /> \n",
    "[ 0.0899347151986333 , 0.17783311623026857 ] <br /> \n",
    "The Confidence Interval of OOB: <br /> \n",
    "[ 14.32721837538061 , 15.775525827409282 ] <br /> \n",
    "<br /> \n",
    "\n",
    "#### TASK 3: \n",
    "\n",
    "Using 30 models by using 30 samples in TASK-1, the predicted output for the given xq was obtained to be 18.9"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bootstrap_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
